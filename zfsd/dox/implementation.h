/**
 *  \file implementation.h
 *
 *  \brief ZlomekFS implementation details
 *  \author based on Josef Zlomek thesis
 */

/*!
   \page zfs-implementation ZlomekFS implementation details

   This page describes the important or interesting parts of
   the implementation in more detail.

   \section architecture  Architecture
   ZlomekFS is User Space file system.

   \subsection communication_between_nodes Communication between Nodes

   \subsubsection communication_protocol Communication Protocol
   Nodes have to invoke operations on the other nodes to be able to access the
   volumes that are not provided by them. Remote procedure call protocols are
   well suited for this purpose.

   A proprietary communication protocol
   is used by zlomekFS. It sends packets over TCP streams so it does not have to
   worry about network errors. The packets use the little endian encoding, the
   elemental data types are aligned to their size, and strings are encoded as
   zero-terminated strings together with their length. The daemon performs
   several optimizations with respect to decoding and encoding of the packets.
   No memory is allocated and no bytes are copied when strings or data buffers
   are decoded, just a pointer is returned from the decoding functions. Similarly,
   the read operation reads the data directly to the packet.

   \see zfs-protocol

   \subsubsection connecting_and_authentication Connecting and Authentication

   When the daemon on one node wants to send a request to another node, it
   checks whether there already is an established connection between these two
   nodes and if so, the existing connection is used. It they are not connected
   and the last attempt to connect was at least a predefined amount of time
   ago the daemon tries to connect.

   After the connection is established, both nodes have to know which node is
   connected to the other end. Although the node that initiated the connection
   already knows which node it wanted to connect, it is good to verify whether
   it has really connected to the expected node to detect bugs in configuration.
   But the node that accepted the connection cannot find out which node has
   connected because nodes are allowed to connect from arbitrary address (this
   is a case of a laptop connected via a modem). So each node sends its name
   to the other node in the authentication messages.

   The daemons should use a reciprocal security check to verify whether
   known nodes are going to be connected, but it is not implemented yet. This
   could be done by authenticating the nodes by Kerberos or by digitally signing
   the random bytes generated by the other node.

   \subsubsection measuring_the_connection_speed  Measuring the Connection Speed

   In order to support mobile operation, the daemon has to estimate the speed
   of connection so that it could limit the network trafic when the node is
   connected via a slow link. Because a node connected to a fast network may
   communicate with a node connected via a slow link, the speed of the link via
   which the node is connected to network cannot be used, so the daemon has to
   measure the speed of network between the two nodes. Latency is measured
   because it is probably a better parameter of the connection than throughput.

   The first idea is to estimate the connection speed while the requests are
   being sent to the other node. The only requests that are being sent in a
   relatively large number are the operations that induce the target node to
   access its disk or send further requests to another node. These actions take
   a relatively long time and would distort the delay between the request and
   reply so they are not suitable for measuring the speed.

   So it is necessary to use an auxiliary request, which is processed by the
   receiver as quickly as possible. The results are probably the most exact when
   the ICMP echo and echo reply messages are used. However, firewalls may be
   configured to drop ICMP trafic thus making this option not well functional.
   Anyway, the TCP connection between the nodes can be used for sending so
   called ping messages. This works well although the overhead of processing
   the message is higher than the delay of processing the ICMP message.

   The speed could be measured in regular time intervals or only when the
   connection is being established. The daemon expects that the connection
   speed does not change while it is established, and that the node has to
   disconnect first to change the quality of connection. This can be expected
   because the change of speed should happen only when the node moves in
   space, for instance when a laptop is taken away and connected later via a
   modem. So the speed is measured only when making the connection.

   \subsection file_handles_and_capabilities File Handles and Capabilities

   Each file system that allows a file to have several hard links has to provide
   low-level file identifiers. The low-level file identifiers are good for other file
   systems too because it is better to identify files internally by numbers and
   not by strings. In Unix file systems, the low-level identifier is usually called
   an inode.

   The distributed file systems that support a remote open have to identify
   the open files too so that clients would not be allowed to read or write the
   files that they did not open.

   \subsubsection file_handles  File Handles

   File handles are the unique identifiers of files. They consist of the ID of the
   node that created the file handle, the ID of the volume on which the file is,
   the device and inode number of the local file in which the file is stored and
   finally the generation number, \ref zfs_fh. The generation is increased
   each time the file in the local file system is deleted and thus a newly created
   file with the same  first four numbers differs in the generation.

   The operations performed on the local file system require the local paths
   to the file handles to be determined. Although ZFS provides a possibility to
   build the local path to the file handle from the information in the metadata,
   it is important to keep the path information in memory because it is much
   faster to create the path from the in-memory data structures than from the
   metadata.

   One of the options is to cache whole paths for file handles. But it would
   be dificult and ineficient to update such a cache. For example, when a
   directory is renamed or moved, it would be necessary to update or invalidate
   the cached paths for its whole subtree. It would be good to have a data
   structure that could handle such situations.

   Fortunately, there is a solution that does not have the problems of the
   previous option. The main idea is to remember the components of the path
   separately and to concatenate the path when it is required. Because the file
   handle may have several hard links, it is necessary to separate the name from
   the in-memory representation of the file handle. The in-memory structure
   holding the name of the file, a pointer to the parent, and a pointer to the
   file handle is called a dentry \ref internal_dentry_def.

   The in-memory file handle \ref internal_fh_def contains the list of
   subdentries because it must be shared between all hard links of the file handle
   when it is possible to make several hard links of a directory. Moreover, the file
   handle is used for synchronization and metadata management. These data
   structures solve the problem of renaming or moving a directory by changing
   the name in the dentry and redirecting several pointers.

       The file system must assign the file handles for special file types too. The
   special files are the virtual directories (the components of paths to volume
   roots \ref virtual_dir_def), the conflict directories and the non-existing files (the deleted files in
   the delete – modify or modify – delete conflicts). ZFS exploits the fact that
   certain numbers cannot be used as IDs of the nodes and volumes so the
   various combinations are assigned to the virtual files. The node ID must not
   be a NODE NONE which is defined to 0 and the volume ID cannot be a
   VOLUME ID VIRTUAL which is also defined to 0. The assignment of node
   and volume IDs is outlined in the following table.

   \verbatim
       Node ID      Volume ID       Type of the file
   	 =0             =0        Virtual directory
   	 =0            <>0        Conflict directory
   	<>0             =0        Non-existing file
   	<>0            <>0        Real file
   \endverbatim

   \subsubsection capabilities Capabilities

   As mentioned earlier, it is necessary to identify open files. They are identified
   by capabilities in ZFS, although ZFS capabilities are slightly different from
   the usual capabilities, which consist of a file identifier, access rights and
   verification. ZFS capabilities consist of the file handle, the flags
   the file was opened with and the verification \ref zfs_cap.

   Each node should be able to verify the capabilities that are sent to the
   node within the requests. So that it could be possible, the capability must
   be generated by the node. When the node is accessing the volume remotely,
   it has to know the capability generated by the volume master too, and the
   capability received in the request must be replaced by the master capability
   before invoking the operation on the volume master. So a mapping from the
   local capability to the master one must be remembered (in memory).

   Because the daemon keeps the local and master capability in memory,
   the verification does not have to be encrypted. It sufices to compute the
   verification as a hash sum from the rest of the capability and from several
   random bytes, and compare the verification of the received capability and
   of the capability kept in memory. When the file is closed, the capability is
   destroyed.

   \subsection creating_and_resolving_the_conflicts Creating and Resolving the Conflicts

   \subsubsection creating_the_conflicts Creating the Conflicts
   When a conflict is discovered, the conflict directory is created and the two
   (local and master) conflicting versions are placed into it. Different types of
   conflicts are detected during different actions.
   
   \verbatim
   attribute – attribute and modify – modify conflicts are found while
        updating the conflicting file or the directory containing the file.
   create – create conflicts are detected during the update of the directory
        containing the conflicting file or during the reintegration of the add log
        entry.

   modify – delete conflicts are found when updating the directory in which
       the file is located.

   delete – modify conflicts are discovered when the corresponding del log
        entry is being reintegrated.
   \endveerbatim

   \subsection resolving_the_conflicts Resolving the Conflicts

   Because a conflict is represented in the file system as a directory containing
   the versions, it is easy to resolve the conflict. To do so, the user can simply
   delete one version and thus select the other to be the correct one. The
   daemon handles the deleting in the conflict directory in a special way.
   \verbatim
      The actions are different for different types of conflicts:

   attribute – attribute conflicts
        The daemon simply sets the attributes of the version being “deleted”
        according to attributes of the other version.
         This type of conflict may also be resolved by changing the attributes
         so that they do not conflict anymore.

   modify – modify conflicts
       When the user “deletes” the local version of the file, the local changes
       have to be lost and the file updated according to the master version.
       So the daemon deletes the list of modified intervals12 of the file and
       the list of updated intervals13 . Finally, it is necessary to change the
       versions so that the file would look as it was not modified by the local
       node and was modified by the volume master, and schedule the file for
       updating.
       Another option would be to simply delete the file and let it completely
       update again but it would result in higher network trafic.
         When the user wants to “delete” the master version, the list of updated
         intervals13 is added to the list of modified intervals12 , and the versions
         are changed to mean that the file was not modified on the volume
         master. Finally, the file is scheduled for reintegration.
         The method above is a need, not an option. The master version of the
         file cannot be simply deleted because the local file does not have to be
         completely updated, so corresponding data would be lost if the master
         version was deleted.

    create – create conflicts
        When resolving this type, the corresponding file is really deleted, or
        moved aside14 if the file with corresponding file handle exists on the
        other node. This makes a place for the other version and the file is
        created in this place when the directory containing the file is being
        updated or reintegrated.

   modify – delete and delete – modify conflicts
       When the user confirms the deletion of the modified file by “deleting”
       it, the daemon really deletes the file.
   	When the user discards the deletion of the modified file by “deleting”
   	the virtual symlink representing the deleted file, the modification log
   	for the directory containing the file is modified to make the daemon
   	update or reintegrate the file next time it verifies the versions of the
   	directory. An add log entry is added when resolving modify – delete
   	conflict to force the file to be reintegrated, or the log entry that caused
   	the delete – modify conflict to be created is deleted.
   \endverbatim


   \subsection invalidating_the_kernel_dentries Invalidating the Kernel Dentries

   When the kernel wants to lookup a file, it tries to find it in the dentry cache.
   If the dentry is found there and is still valid, it is used. Otherwise the kernel
   invokes a lookup operation and creates a new dentry.

   So when the daemon knows that a directory entry was deleted or moved,
   it is necessary to invalidate the dentry in the cache. Although the kernel
   would invalidate the dentry automatically after some time, it is necessary to
   invalidate it as soon as possible because the file should disappear immediately.

   When the daemon deletes, renames or moves a file, creates or resolves a
   conflict, its sends a one-way invalidation request to the kernel so that the
   view of the file system presented to a user would be up-to-date.

   \subsection metadata Metadata

   The ZFS file system requires more metadata than the local file system. The
   additional information is stored in the directory .zfs in the root of the
   volume in the local file system, this directory is invisible via ZFS.

   Using an auxiliary file for each file of the file system would take too much
   space because a whole block is usually allocated for small files. This can be
   easily avoided for fixed length metadata like file handle, local version, master
   version etc. These fixed length metadata are stored in a hashed file \ref hashfile, which
   automatically grows and shrinks. There is one more hashed file that contains
   the mapping of master file handle to the local one.

   Unfortunately, all types of metadata do not have a fixed length. Such
   metadata are stored in separate files whose names are generated from the
   file handles and the types of metadata. To avoid the eficiency limits of
   directories15 of the local file system, these files are stored in an appropriate
   directory of the directory tree whose components also have names generated
   from the file handles.

   The fist type of variable-length metadata is the list of hard links of the
   file. Each hard link is represented by its file name and a part of the file
   handle of the directory containing the file. However, if the file has only one
   hard link and its name is short enough, which is actually the most common
   case, the information about hard link is stored in the hashed file and not in
   the additional file.

   The various modification logs, which describe the changes made by the
   local node and are used during the reintegration and update, are stored in
   separate files.
   The types of modification logs are described in the following sections because they are
   important and interesting optimizations are performed on them.

       An example of locations, as seen from the volume root, of various types

   \verbatim
   .zfs/metadata
   .zfs/fh_mapping
   .zfs/3/5/000003030013C053.modified
   .zfs/3/7/000003030013C073.updated
   .zfs/5/E/000003030013C0E5.hardlinks
   .zfs/D/A/000003030013C0AD00000001.journal
   \endverbatim

   \subsubsection journal_for_directories  Journals for Directories

   There is a separate journal for each directory that contains the changes that
   have not been reintegrated yet. There are two types (\ref journal_operation_t)
   of journal records, the add record and del record that mean
   that a directory entry was added or deleted, respectively.

   Each record (\ref journal_entry_def)  contains the name and the file handle of the added or deleted
   directory entry. It also contains the master file handle so that the file system
   would know what file it should link, move or delete on the volume master or
   could detect create – create conflicts, and also the master version so that
   it could detect delete – modify conflicts.

   The journal can be optimized for size. When there is an add record and
   later a del record with the same name in the journal, such a pair of records
   may be deleted without loss of information. There remain only records for
   files that originally were in the directory and were deleted, and for files that
   did not exist and were added to the directory. So the space complexity of
   the optimized journal is O(n1 + n2 ) where n1 is the original number of files
   after last reintegration and n2 is the current number of files in the directory.

   When the daemon creates an in-memory file handle for the directory, it
   reads the corresponding journal into a special data structure (\ref journal_def). The data
   structure keeps the journal optimized when entries are being added to it.
   The optimized journal is written back to the file after it is read to save the
   disk space. When the daemon wants to add a new entry to the journal, it
   appends the record to the journal file and inserts it to the data structure.
   During the reintegration, the reintegrated journal entries get deleted so the
   modified optimized journal has to be written to the file again.

   \subsubsection interval_files Interval Files

   For each regular file, there is a file containing the list of modified intervals
   of the file (i.e. the modification log) and a file with the list of intervals that
   were updated according to the volume master in the metadata directory tree.
   If the file was not modified or it was completely updated, the corresponding
   interval file does not exist to save the disk space.

   The corresponding interval files are read (\ref interval_tree_read) when the regular file is being
   opened. The intervals are added to an interval tree (\ref interval_tree_def) that merges the adjacent
   intervals (\ref interval_def) and unions the overlapping ones and thus consolidates the interval
   list. The consolidated interval list is written to the file after the complete
   original list has been read to save disk space. When a new interval is to be
   added, it is appended to the interval file and inserted to the interval tree.

   When the daemon is closing the regular file, the optimized interval lists are
   written (\ref interval_tree_write) to interval files again.

   \subsection shadow_directory  Shadow Directory

   When a file or directory is being moved from one directory to another one,
   a del journal entry is added to the source directory and an add entry to the
   destination directory. If the del entry is being reintegrated first it is necessary
   to make the directory entry available but keep the file or directory aside until
   the add entry gets reintegrated, which will move the file or directory to its
   new place. The place where the file or directory is located meanwhile is called
   a shadow directory.

   The files or directories are being moved to the shadow directory in one
   more situation. When a node is updating a parent directory of directory D
   and directory D was deleted by the volume master, the directory D is deleted
   during the update. However, the node might have created or modified files
   or create directories in the subtree of directory D but such files or directories
   should not be deleted. It would be ineficient to check the whole contents of
   directory D, and create a conflict in this situation, so the new and changed
   files and the new directories are moved to the shadow directory. The user
   has to manually check whether these files really should be deleted.

   The shadow directory is a directory .shadow in the root of each volume.
   The shadow directory is visible via zlomekFS only to the local node so that the user
   could access its contents and move the files or directories to an appropriate
   place. The files and directories are stored in a directory tree similar to the
   directory tree for variable-length metadata and their names consist of the
   original name and the file handle to avoid name conflicts.

   \subsection configuration Configuration

   The configuration consists of the global configuration, which is shared by all
   nodes and is stored in the configuration volume, and of the
   local configuration, which is private to each node.

   \subsubsection local_configuration  Local Configuration

   The local configuration contains information that is specific to a given node
   and should not be accessible by other nodes. It consists of one file.

  \verbatim
  zfsd.conf 
       This is the local configuration file, which is usually located in
       /etc/zfs/zfsd.conf. If it has a different location it must be specified in
       the -o config=FILE option of the daemon.
       
          The file format is based on libconfig http://www.hyperrealm.com/libconfig/.
          The supported configuration directives are listed below.

          # Example ZlomekFS configuration file
          # configuration version
          version = "1.0";

          # configuration of local node
	  # The local node needs this information to connect to another node to read the global configuration.
          local_node:
          {
          	name = "the_only_node";
          	id = 1;
          };

          # volume list
	  # Holds the information about volumes provided or cached by the local node.
 	  # file system, and finally the limit of the size of the cached volume19 in
 	  # the format ID:PATH:LIMIT.
          volumes:
          (
	  # each entry for each volume
          { 
		# volume id
          	id = 1;
		# limit of the cached volume
          	cache_size = 0;
		# path to local cache
          	local_path = "/var/zfs/config";
          },
          {
          	id = 2;
          	local_path = "/var/zfs/data";
          	cache_size = 0;
          }
          );

          # system specific configuratiom
          system:
          {
          	mlock = false;
          	# the depth of the directory tree containing the files with variable-length metadata, the default is 1.
          	metadata_tree_depth = 1;
          	local_config = "/var/zfs/config";
          };

          threads:
          {
          	# max_total = 10;
          	# min_spare = 10;
          	# max_spare = 10;
          };

          users:
          {
		# The name of the local user. When a mapping for the file system UID is not defined, a mapping to this user is used. The default is nobody.
          	default_user = "nobody";
		# The ID of the local user. This directive is an alternative to the default_user, the default is the UID of nobody.
          #	default_uid = 65534;
          };

          groups:
          {
		# The name of the local group. This directive is similar to the default_user, the default is nogroup or nobody.
          	default_group = "nogroup";
		# The ID of the local group. This is an alternative to the default_group, the default is the GID of nogroup or nobody.
 		# default_gid = 65534;
          };
   \endverbatim
 

   \subsubsection global_configuration Global Configuration

   The global configuration contains the information that is shared by all nodes.
   It consists of several files stored in the configuration volume, which is cached
   by all nodes and is automatically updated when it changes.
   Format of all files of the global configuration is based on libconfig http://www.hyperrealm.com/libconfig/.

   \verbatim
   	configuration volume example

	* node_list
	       # Nodes are listed in this file.
		node:
		{
			list=
			(
			# each entry for each node
			{
				# ID of the node
				id=1;
				# name of the node
				name="the_only_node";
				# host name of the node
				address="127.0.0.1";
				# port of the node, optional default is 12325
				port="12325";
			}
			);
		};

	* volume_list
		# There is a list of volumes in this file.
		volume:
		{
			# a list of volumes
			list =
			(
				# each entry for each volume
				{
					# volume id
					id = 1;
					# volume name
					name = "config";
					# volume mountpoint
					mountpoint="/config";
				},
				{
					id = 2;
					name = "data";
					mountpoint="/data";
				}
			);
			# desctibes hierarchy of volumes
			layout =
			(
				# each entry describes hierarchy for each volume
				{
					# volume name
					volume = "config";
					# hierarchy of the corresponding volume
					tree =
					{
						node="the_only_node";
						# list of subtrees
						children = ();
					};
					# example of hierarchy form Josef Zlomek thesis
					#  tree =
					#  {
					#  	node="N1";
					#  	children = (
					#  		{
					#  			node = "N2";
					#  			children = ();
					#  		},
					#  		{
					#  			node = "N3";
					#  			children = (
					#  				{
					#  					node = "N4";
					#  					children = ();
					#  				},
					#  				{
					#  					node = "N5";
					#  					children = ();
					#  				},
					#  				{
					#  					node = "N6";
					#  					children = ();
					#  				},
					#  			);
					#  		},
					#  		{
					#  			node = "N7";
					#  			children = (
					#  				{
					#  					node = "N8";
					#  					children = ();
					#  				},
					#  				{
					#  					node = "N9";
					#  					children = ();
					#  				},
					#  			);
					#  		}
					#  	)
					#  }
				},
				{
					volume = "data";
					{
						node="the_only_node";
						# list of subtrees
						children = ();
					};

				}
			);

		};

	* user_list:
		# Users are listed in this file.
		user:
		{
			list = 
			(
				# each entry for each user
				{
					# user ID
					id = 65534;
					# user name
					name = "nobody";
				}
			);
		};

	* group_list:
		# Groups are listed in this file.
		group:
		{
			# group id and group name list
			list = 
			(
				{
					# group id
					id = 65534;
					# group name
					name = "nogroup";
				}
			);
		};

	* user/
		The files in this directory describe the mapping between the file system
		user IDs and the node user IDs. The name of almost every file is the
		name of the node for which the mapping is. The only exception is the
		file whose name is default, which holds the default user ID mapping,
		i.e. if a mapping for a given user is not found in the mapping for
		the corresponding node the default mapping is used.
	* user/default:
		# Default mapping
		user:
		{
			mapping = 
			(
				{
					# for which node is this mapping
					node = "default";
					pairs =
					(
						{
							# local user name
							local = "nobody";
							# remote user name
							remote = "nobody";
						}
					);
				}
			);
		};

	* group/
	       This directory is almost the same as the user directory, the difference
	       is that the files in this directory describe the mapping of groups instead
	       of users.
	* group/default:
		# group configuration
		group:
		{
			# mapping between local and remote node
			mapping = 
			(
				{
					# default is reserved name for default configuration, don't use default for node name
					node="default";
					pairs=
					(
						{
							local="nogroup";
							remote="nogroup";
						}
					);
				}
			);
		};

   \endverbatim

   For more information about implementation see \ref zfs-configuration.
*/
